{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679cc5bc",
   "metadata": {},
   "source": [
    "# Spark Optimization Notebook\n",
    "\n",
    "This notebook demonstrates practical examples and code snippets to optimize Spark jobs for better performance based on partitioning, shuffle operations, memory management, and Adaptive Query Execution (AQE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c697ee2",
   "metadata": {},
   "source": [
    "## 1. Understanding Partitioning\n",
    "Partitioning allows Spark to divide data into smaller, manageable chunks for parallel processing. Proper partitioning can improve task parallelism and reduce shuffling.\n",
    "\n",
    "**Best Practices for Partitioning:**\n",
    "- Use `repartition(n)` to increase partitions for large datasets.\n",
    "- Use `coalesce(n)` to decrease partitions for smaller datasets.\n",
    "- Leverage data locality to minimize shuffle overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f53e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adjusting Partitions\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Optimization Example\").getOrCreate()\n",
    "\n",
    "# Create a large dataset\n",
    "data = [(x, x**2) for x in range(1000000)]\n",
    "df = spark.createDataFrame(data, [\"number\", \"square\"])\n",
    "\n",
    "# Repartition to optimize parallelism\n",
    "df = df.repartition(100)\n",
    "print(\"Number of partitions after repartition:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b8503",
   "metadata": {},
   "source": [
    "## 2. Diagnosing Performance Bottlenecks\n",
    "Use `explain()` to analyze query execution plans and identify inefficiencies such as excessive shuffling or skewed partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ca3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Execution Plan\n",
    "df.groupBy(\"number\").count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f8cbb",
   "metadata": {},
   "source": [
    "## 3. Optimizing Shuffle Operations\n",
    "Shuffling involves data movement across partitions, which can be expensive. The following strategies can reduce shuffle overhead:\n",
    "\n",
    "- Use `reduceByKey` instead of `groupByKey` for aggregations.\n",
    "- Avoid wide transformations where possible.\n",
    "- Enable **salting** to mitigate data skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reduce Shuffle with reduceByKey\n",
    "rdd = spark.sparkContext.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "reduced_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(reduced_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6366f1a",
   "metadata": {},
   "source": [
    "### Mitigating Data Skew with Salting\n",
    "Data skew can cause performance bottlenecks. Salting is a technique to distribute data more evenly across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ecfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Salting for Skew Mitigation\n",
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "# Add a salt column to distribute keys\n",
    "salted_df = df.withColumn(\"salt\", (col(\"number\") % 10).cast(\"string\"))\n",
    "salted_df = salted_df.withColumn(\"salted_key\", concat(col(\"number\"), lit(\"_\"), col(\"salt\")))\n",
    "salted_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c88aa7",
   "metadata": {},
   "source": [
    "## 4. Adaptive Query Execution (AQE)\n",
    "Adaptive Query Execution dynamically optimizes execution plans at runtime, addressing issues such as skewed partitions and suboptimal join strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Execute a query with AQE enabled\n",
    "df.groupBy(\"number\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62731fdf",
   "metadata": {},
   "source": [
    "## 5. Memory Management\n",
    "Efficient memory management is critical for Spark performance. Best practices include:\n",
    "\n",
    "- Adjusting executor and driver memory (`spark.executor.memory`, `spark.driver.memory`).\n",
    "- Caching intermediate results to avoid recomputation.\n",
    "- Using `persist()` for repeated access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Caching DataFrames\n",
    "cached_df = df.cache()\n",
    "cached_df.count()  # Triggers caching\n",
    "\n",
    "# Persist with specific storage levels\n",
    "persisted_df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298e67a",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices\n",
    "- **Partitioning**: Adjust partitions for parallelism and data locality.\n",
    "- **Shuffling**: Avoid unnecessary wide transformations and use salting to mitigate skew.\n",
    "- **AQE**: Enable dynamic query optimization for runtime adjustments.\n",
    "- **Memory Management**: Use caching and persisting for efficient resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ddf1c-175d-4521-b648-5c7ce65329eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
