{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0554fe1f",
   "metadata": {},
   "source": [
    "# Spark K-Means Clustering Example\n",
    "\n",
    "This notebook demonstrates how to use the K-Means clustering algorithm in PySpark's MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ececf",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Spark Session\n",
    "First, we create a Spark session to work with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3db8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/13 20:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/13 20:18:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/13 20:18:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/13 20:18:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansClustering\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead417fa",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "We create a simple dataset for clustering. This dataset contains points in a 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff400fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id|  x|  y|\n",
      "+---+---+---+\n",
      "|  1|1.0|1.0|\n",
      "|  2|1.5|1.5|\n",
      "|  3|3.0|3.0|\n",
      "|  4|5.0|5.0|\n",
      "|  5|3.5|3.5|\n",
      "|  6|4.5|4.5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a sample dataset\n",
    "data = [\n",
    "    Row(id=1, x=1.0, y=1.0),\n",
    "    Row(id=2, x=1.5, y=1.5),\n",
    "    Row(id=3, x=3.0, y=3.0),\n",
    "    Row(id=4, x=5.0, y=5.0),\n",
    "    Row(id=5, x=3.5, y=3.5),\n",
    "    Row(id=6, x=4.5, y=4.5)\n",
    "]\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9aa680",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "We use `VectorAssembler` to combine the feature columns (`x` and `y`) into a single vector column required by the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140567c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+\n",
      "| id|  x|  y| features|\n",
      "+---+---+---+---------+\n",
      "|  1|1.0|1.0|[1.0,1.0]|\n",
      "|  2|1.5|1.5|[1.5,1.5]|\n",
      "|  3|3.0|3.0|[3.0,3.0]|\n",
      "|  4|5.0|5.0|[5.0,5.0]|\n",
      "|  5|3.5|3.5|[3.5,3.5]|\n",
      "|  6|4.5|4.5|[4.5,4.5]|\n",
      "+---+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "dataset = assembler.transform(df)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31f90c",
   "metadata": {},
   "source": [
    "## Step 4: Apply K-Means Clustering\n",
    "We apply the K-Means algorithm to cluster the data points into two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cb5264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/13 20:19:01 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+-------+\n",
      "| id|  x|  y| features|cluster|\n",
      "+---+---+---+---------+-------+\n",
      "|  1|1.0|1.0|[1.0,1.0]|      1|\n",
      "|  2|1.5|1.5|[1.5,1.5]|      1|\n",
      "|  3|3.0|3.0|[3.0,3.0]|      0|\n",
      "|  4|5.0|5.0|[5.0,5.0]|      0|\n",
      "|  5|3.5|3.5|[3.5,3.5]|      0|\n",
      "|  6|4.5|4.5|[4.5,4.5]|      0|\n",
      "+---+---+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Create and train the K-Means model\n",
    "kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28e726",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Clustering Model\n",
    "We evaluate the model by computing the **Within Set Sum of Squared Errors (WSSSE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b101a4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeansModel' object has no attribute 'computeCost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate clustering by computing WSSSE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wssse \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeCost\u001b[49m(dataset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithin Set Sum of Squared Errors (WSSSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwssse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KMeansModel' object has no attribute 'computeCost'"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing WSSSE\n",
    "wssse = model.computeCost(dataset)\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe402d3",
   "metadata": {},
   "source": [
    "## Step 6: Extract Cluster Centers\n",
    "We extract and display the cluster centers determined by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d79a8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[4. 4.]\n",
      "[1.25 1.25]\n"
     ]
    }
   ],
   "source": [
    "# Display cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b76c0",
   "metadata": {},
   "source": [
    "## Step 7: Stop the Spark Session\n",
    "Finally, we stop the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212367bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29080a-279e-4007-8fe5-7ee57d18f434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
