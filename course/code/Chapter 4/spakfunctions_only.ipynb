{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame-Only Operations in Spark\n",
    "\n",
    "This notebook provides a detailed tutorial on operations that can only be performed using Spark DataFrame functions, not with SQL alone. We'll explore custom User-Defined Functions (UDFs), chaining transformations, and complex conditional expressions using `when` and `otherwise`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To start, make sure you have PySpark installed. If not, install it by running:\n",
    "```python\n",
    "!pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize SparkSession\n",
    "\n",
    "The first step is to create a SparkSession, which is the entry point for working with Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 14:43:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/02 14:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/02 14:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/02 14:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/02 14:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/02 14:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Only Operations Tutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Sample DataFrame\n",
    "\n",
    "We'll create a sample DataFrame to use for the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  1|Alice|      Sales| 50000| 29|\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "|  5|  Eva|  Marketing| 45000| 23|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Sales\", 50000, 29),\n",
    "    (2, \"Bob\", \"Engineering\", 60000, 35),\n",
    "    (3, \"Cathy\", \"Sales\", 55000, 30),\n",
    "    (4, \"David\", \"Engineering\", 65000, 40),\n",
    "    (5, \"Eva\", \"Marketing\", 45000, 23)\n",
    "]\n",
    "columns = [\"ID\", \"Name\", \"Department\", \"Salary\", \"Age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame-Only Operation 1: Custom User-Defined Functions (UDFs)\n",
    "\n",
    "Spark DataFrames allow us to use custom Python functions with UDFs, which are not supported in SQL. UDFs are especially useful for complex transformations that go beyond SQL's capabilities.\n",
    "\n",
    "In this example, we'll define a UDF to categorize employees as \"Young\" or \"Experienced\" based on their `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+------------+\n",
      "| ID| Name| Department|Salary|Age|Age_Category|\n",
      "+---+-----+-----------+------+---+------------+\n",
      "|  1|Alice|      Sales| 50000| 29|       Young|\n",
      "|  2|  Bob|Engineering| 60000| 35| Experienced|\n",
      "|  3|Cathy|      Sales| 55000| 30| Experienced|\n",
      "|  4|David|Engineering| 65000| 40| Experienced|\n",
      "|  5|  Eva|  Marketing| 45000| 23|       Young|\n",
      "+---+-----+-----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a Python function to categorize age\n",
    "def age_category(age):\n",
    "    return \"Young\" if age < 30 else \"Experienced\"\n",
    "\n",
    "# Convert the Python function to a UDF\n",
    "age_category_udf = udf(age_category, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column\n",
    "df_with_category = df.withColumn(\"Age_Category\", age_category_udf(col(\"Age\")))\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame-Only Operation 2: Chaining Transformations\n",
    "\n",
    "DataFrames allow you to chain multiple transformations in a single expression, which is not as seamless in SQL. Chaining makes the code more readable and efficient.\n",
    "\n",
    "In this example, we’ll apply several transformations in a chain: filtering employees older than 30, adding a 10% bonus to their salary, and selecting specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------+\n",
      "| Name|Age|Salary| Bonus|\n",
      "+-----+---+------+------+\n",
      "|  Bob| 35| 60000|6000.0|\n",
      "|David| 40| 65000|6500.0|\n",
      "+-----+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain transformations in a single expression\n",
    "df_chained = df.filter(col(\"Age\") > 30) \\\n",
    "    .withColumn(\"Bonus\", col(\"Salary\") * 0.1) \\\n",
    "    .select(\"Name\", \"Age\", \"Salary\", \"Bonus\")\n",
    "\n",
    "df_chained.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame-Only Operation 3: Conditional Expressions with `when` and `otherwise`\n",
    "\n",
    "DataFrames support complex conditional expressions using `when` and `otherwise` functions, allowing you to apply different transformations based on conditions, which is cumbersome in SQL.\n",
    "\n",
    "In this example, we’ll create a new column `High_Salary` that marks employees with a salary greater than 55000 as \"Yes\" and others as \"No\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+-----------+\n",
      "| ID| Name| Department|Salary|Age|High_Salary|\n",
      "+---+-----+-----------+------+---+-----------+\n",
      "|  1|Alice|      Sales| 50000| 29|         No|\n",
      "|  2|  Bob|Engineering| 60000| 35|        Yes|\n",
      "|  3|Cathy|      Sales| 55000| 30|         No|\n",
      "|  4|David|Engineering| 65000| 40|        Yes|\n",
      "|  5|  Eva|  Marketing| 45000| 23|         No|\n",
      "+---+-----+-----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use when and otherwise to create conditional column\n",
    "df_with_condition = df.withColumn(\n",
    "    \"High_Salary\",\n",
    "    when(col(\"Salary\") > 55000, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "df_with_condition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame-Only Operation 4: Using Expressions (`expr`)\n",
    "\n",
    "The `expr` function lets us use Spark SQL expressions within the DataFrame API, which is useful for performing arithmetic operations and string manipulations directly within the code.\n",
    "\n",
    "In this example, we’ll calculate a new column `Adjusted_Salary` where we apply a 5% increase to the salary if the employee is in the `Sales` department, and no change otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+---------------+\n",
      "| ID| Name| Department|Salary|Age|Adjusted_Salary|\n",
      "+---+-----+-----------+------+---+---------------+\n",
      "|  1|Alice|      Sales| 50000| 29|       52500.00|\n",
      "|  2|  Bob|Engineering| 60000| 35|       60000.00|\n",
      "|  3|Cathy|      Sales| 55000| 30|       57750.00|\n",
      "|  4|David|Engineering| 65000| 40|       65000.00|\n",
      "|  5|  Eva|  Marketing| 45000| 23|       45000.00|\n",
      "+---+-----+-----------+------+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Use expr to apply a conditional operation\n",
    "df_with_adjusted_salary = df.withColumn(\n",
    "    \"Adjusted_Salary\",\n",
    "    expr(\"CASE WHEN Department = 'Sales' THEN Salary * 1.05 ELSE Salary END\")\n",
    ")\n",
    "df_with_adjusted_salary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1: Create a UDF for Customized Greeting\n",
    "\n",
    "Define a UDF named `greeting` that creates a customized greeting for each employee based on their name and department. For example, \"Hello Alice from Sales!\".\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+--------------------+\n",
      "| ID| Name| Department|Salary|Age|            Greeting|\n",
      "+---+-----+-----------+------+---+--------------------+\n",
      "|  1|Alice|      Sales| 50000| 29|Hello Alice from ...|\n",
      "|  2|  Bob|Engineering| 60000| 35|Hello Bob from En...|\n",
      "|  3|Cathy|      Sales| 55000| 30|Hello Cathy from ...|\n",
      "|  4|David|Engineering| 65000| 40|Hello David from ...|\n",
      "|  5|  Eva|  Marketing| 45000| 23|Hello Eva from Ma...|\n",
      "+---+-----+-----------+------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Define and apply the greeting UDF\n",
    "def greeting(name, department):\n",
    "    return f\"Hello {name} from {department}!\"\n",
    "\n",
    "greeting_udf = udf(greeting, StringType())\n",
    "\n",
    "df_with_greeting = df.withColumn(\"Greeting\", greeting_udf(col(\"Name\"), col(\"Department\")))\n",
    "df_with_greeting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2: Add a Conditional Bonus Column\n",
    "\n",
    "Add a column `Conditional_Bonus` that provides a 10% bonus to employees in `Engineering` and a 5% bonus for employees in other departments. Use the `when` and `otherwise` functions.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+-----------------+\n",
      "| ID| Name| Department|Salary|Age|Conditional_Bonus|\n",
      "+---+-----+-----------+------+---+-----------------+\n",
      "|  1|Alice|      Sales| 50000| 29|           2500.0|\n",
      "|  2|  Bob|Engineering| 60000| 35|           6000.0|\n",
      "|  3|Cathy|      Sales| 55000| 30|           2750.0|\n",
      "|  4|David|Engineering| 65000| 40|           6500.0|\n",
      "|  5|  Eva|  Marketing| 45000| 23|           2250.0|\n",
      "+---+-----+-----------+------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Create Conditional_Bonus column\n",
    "df_with_conditional_bonus = df.withColumn(\n",
    "    \"Conditional_Bonus\",\n",
    "    when(col(\"Department\") == \"Engineering\", col(\"Salary\") * 0.1).otherwise(col(\"Salary\") * 0.05)\n",
    ")\n",
    "df_with_conditional_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3: Chain Multiple Transformations\n",
    "\n",
    "1. Filter employees under 35.\n",
    "2. Add a new column `High_Salary` (marking salaries over 55000 as \"Yes\" or \"No\").\n",
    "3. Select only `Name`, `Age`, `High_Salary`, and `Department`.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+----------+\n",
      "| Name|Age|High_Salary|Department|\n",
      "+-----+---+-----------+----------+\n",
      "|Alice| 29|         No|     Sales|\n",
      "|Cathy| 30|         No|     Sales|\n",
      "|  Eva| 23|         No| Marketing|\n",
      "+-----+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Chain multiple transformations\n",
    "df_transformed = df.filter(col(\"Age\") < 35) \\\n",
    "    .withColumn(\"High_Salary\", when(col(\"Salary\") > 55000, \"Yes\").otherwise(\"No\")) \\\n",
    "    .select(\"Name\", \"Age\", \"High_Salary\", \"Department\")\n",
    "\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we covered key operations that are exclusive to DataFrames in Spark and cannot be done with SQL alone:\n",
    "\n",
    "- **Custom User-Defined Functions (UDFs)**: Ideal for applying Python-based transformations.\n",
    "- **Chaining Transformations**: Enables fluent, readable transformations.\n",
    "- **Conditional Expressions**: Complex conditions handled easily with `when` and `otherwise`.\n",
    "- **Using `expr`**: Integrates SQL expressions directly into DataFrame transformations.\n",
    "\n",
    "These DataFrame-specific functions allow for more flexibility and Python integration, making them invaluable for complex data engineering tasks in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
