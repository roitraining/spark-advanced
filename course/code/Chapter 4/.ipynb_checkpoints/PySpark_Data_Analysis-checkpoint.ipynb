{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d47084",
   "metadata": {},
   "source": [
    "# Data Analysis with PySpark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90627515",
   "metadata": {},
   "source": [
    "In this lab, you'll learn how to: \n",
    "- Set up a Jupyter Notebook for PySpark.\n",
    "- Load data from various formats (JSON, CSV, Parquet, Avro).\n",
    "- Perform SQL queries and data analysis using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31db41",
   "metadata": {},
   "source": [
    "## Step 1: Set Up PySpark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessingLab\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7045bbe",
   "metadata": {},
   "source": [
    "## Step 2: Load Data from Different Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048ccfa",
   "metadata": {},
   "source": [
    "### 1. Reading JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "json_df = spark.read.json(\"employees.json\")\n",
    "json_df.show()\n",
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44863411",
   "metadata": {},
   "source": [
    "### 2. Reading CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d297da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "csv_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "csv_df.show()\n",
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b6d8a",
   "metadata": {},
   "source": [
    "### 3. Reading Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV DataFrame as Parquet and read it back\n",
    "csv_df.write.mode(\"overwrite\").parquet(\"employees.parquet\")\n",
    "parquet_df = spark.read.parquet(\"employees.parquet\")\n",
    "parquet_df.show()\n",
    "parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba071a16",
   "metadata": {},
   "source": [
    "### 4. Reading Avro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as Avro and read it (requires spark-avro package)\n",
    "parquet_df.write.mode(\"overwrite\").format(\"avro\").save(\"employees.avro\")\n",
    "avro_df = spark.read.format(\"avro\").load(\"employees.avro\")\n",
    "avro_df.show()\n",
    "avro_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014acc1",
   "metadata": {},
   "source": [
    "## Step 3: Create Temporary Views and Run SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views\n",
    "json_df.createOrReplaceTempView(\"employees_json\")\n",
    "csv_df.createOrReplaceTempView(\"employees_csv\")\n",
    "parquet_df.createOrReplaceTempView(\"employees_parquet\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"SELECT name, department, salary FROM employees_json WHERE salary > 55000\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1fa17",
   "metadata": {},
   "source": [
    "## Step 4: Perform Data Analysis Using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employees who joined after 2021\n",
    "recent_employees = json_df.filter(json_df.join_date > \"2021-01-01\")\n",
    "recent_employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average salary by department\n",
    "avg_salary = csv_df.groupBy(\"department\").avg(\"salary\")\n",
    "avg_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort employees by salary in descending order\n",
    "sorted_employees = parquet_df.orderBy(parquet_df.salary.desc())\n",
    "sorted_employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c4824",
   "metadata": {},
   "source": [
    "## Step 5: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "result.write.mode(\"overwrite\").csv(\"filtered_employees.csv\")\n",
    "\n",
    "# Save as JSON\n",
    "avg_salary.write.mode(\"overwrite\").json(\"average_salary.json\")\n",
    "\n",
    "# Save as Parquet\n",
    "avg_salary.write.mode(\"overwrite\").parquet(\"average_salary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd5ad5",
   "metadata": {},
   "source": [
    "### Lab Summary\n",
    "In this lab, you've learned how to:\n",
    "- Set up PySpark in Jupyter Notebook.\n",
    "- Read data from different file formats.\n",
    "- Perform SQL queries and DataFrame operations.\n",
    "- Save results in different formats."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
