{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate rows via join\n",
    "\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns.\n",
    "\n",
    "The spark object is available, and pyspark.sql.functions is imported as F.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "- Rename the _c0 column to folder on the valid_folders_df DataFrame.\n",
    "- Count the number of rows in split_df.\n",
    "- Join the two DataFrames on the folder name, and call the resulting DataFrame joined_df. Make sure to broadcast the smaller DataFrame.- \n",
    "- Check the number of rows remaining in the DataFrame and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_folders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Rename the column in valid_folders_df\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m valid_folders_df \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_folders_df\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_c0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Count the number of rows in split_df\u001b[39;00m\n\u001b[1;32m      5\u001b[0m split_count \u001b[38;5;241m=\u001b[39m split_df\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_folders_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining invalid rows\n",
    "\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows.\n",
    "\n",
    "The spark object is defined and pyspark.sql.functions are imported as F. The original DataFrame split_df and the joined DataFrame joined_df are available as they were in their previous states.\n",
    "\n",
    "### Instructions\n",
    "- Determine the row counts for each DataFrame.\n",
    "- Create a DataFrame containing only the invalid rows.\n",
    "- Validate the count of the new DataFrame is as expected.- \n",
    "- Determine the number of distinct folder rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
