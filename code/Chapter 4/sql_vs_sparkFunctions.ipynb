{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation in Spark: SQL vs DataFrame Functions\n",
    "\n",
    "This notebook provides a comprehensive comparison of data manipulation in Spark using SQL and built-in DataFrame functions. For each task, we’ll show both methods to perform the operation, so you can see how they compare in syntax and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize SparkSession\n",
    "\n",
    "The first step is to create a SparkSession, which is the entry point for working with Spark SQL and DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 14:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/02 14:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/02 14:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/02 14:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/02 14:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/02 14:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, avg, sum\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SQL vs DataFrame Tutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Sample DataFrame\n",
    "\n",
    "We'll create a sample DataFrame that we can use for various manipulation tasks throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  1|Alice|      Sales| 50000| 29|\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "|  5|  Eva|  Marketing| 45000| 23|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Sales\", 50000, 29),\n",
    "    (2, \"Bob\", \"Engineering\", 60000, 35),\n",
    "    (3, \"Cathy\", \"Sales\", 55000, 30),\n",
    "    (4, \"David\", \"Engineering\", 65000, 40),\n",
    "    (5, \"Eva\", \"Marketing\", 45000, 23)\n",
    "]\n",
    "columns = [\"ID\", \"Name\", \"Department\", \"Salary\", \"Age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Selecting Specific Columns\n",
    "\n",
    "Let's select the `Name` and `Salary` columns using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|Salary|\n",
      "+-----+------+\n",
      "|Alice| 50000|\n",
      "|  Bob| 60000|\n",
      "|Cathy| 55000|\n",
      "|David| 65000|\n",
      "|  Eva| 45000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "spark.sql(\"SELECT Name, Salary FROM employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|Salary|\n",
      "+-----+------+\n",
      "|Alice| 50000|\n",
      "|  Bob| 60000|\n",
      "|Cathy| 55000|\n",
      "|David| 65000|\n",
      "|  Eva| 45000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.select(\"Name\", \"Salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Filtering Rows\n",
    "\n",
    "Filter employees with a `Salary` greater than 50000 using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"SELECT * FROM employees WHERE Salary > 50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.filter(col(\"Salary\") > 50000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Aggregation and Grouping\n",
    "\n",
    "Find the average salary by `Department` using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "| Department|Avg_Salary|\n",
      "+-----------+----------+\n",
      "|      Sales|   52500.0|\n",
      "|Engineering|   62500.0|\n",
      "|  Marketing|   45000.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"SELECT Department, AVG(Salary) AS Avg_Salary FROM employees GROUP BY Department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "| Department|Avg_Salary|\n",
      "+-----------+----------+\n",
      "|      Sales|   52500.0|\n",
      "|Engineering|   62500.0|\n",
      "|  Marketing|   45000.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Avg_Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sorting Data\n",
    "\n",
    "Sort employees by `Salary` in descending order using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  1|Alice|      Sales| 50000| 29|\n",
      "|  5|  Eva|  Marketing| 45000| 23|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"SELECT * FROM employees ORDER BY Salary DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+\n",
      "| ID| Name| Department|Salary|Age|\n",
      "+---+-----+-----------+------+---+\n",
      "|  4|David|Engineering| 65000| 40|\n",
      "|  2|  Bob|Engineering| 60000| 35|\n",
      "|  3|Cathy|      Sales| 55000| 30|\n",
      "|  1|Alice|      Sales| 50000| 29|\n",
      "|  5|  Eva|  Marketing| 45000| 23|\n",
      "+---+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.orderBy(col(\"Salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Adding a Calculated Column\n",
    "\n",
    "Add a new column `Bonus` which is 10% of `Salary`, using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+------+\n",
      "| ID| Name| Department|Salary|Age| Bonus|\n",
      "+---+-----+-----------+------+---+------+\n",
      "|  1|Alice|      Sales| 50000| 29|5000.0|\n",
      "|  2|  Bob|Engineering| 60000| 35|6000.0|\n",
      "|  3|Cathy|      Sales| 55000| 30|5500.0|\n",
      "|  4|David|Engineering| 65000| 40|6500.0|\n",
      "|  5|  Eva|  Marketing| 45000| 23|4500.0|\n",
      "+---+-----+-----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"SELECT *, Salary * 0.1 AS Bonus FROM employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+------+\n",
      "| ID| Name| Department|Salary|Age| Bonus|\n",
      "+---+-----+-----------+------+---+------+\n",
      "|  1|Alice|      Sales| 50000| 29|5000.0|\n",
      "|  2|  Bob|Engineering| 60000| 35|6000.0|\n",
      "|  3|Cathy|      Sales| 55000| 30|5500.0|\n",
      "|  4|David|Engineering| 65000| 40|6500.0|\n",
      "|  5|  Eva|  Marketing| 45000| 23|4500.0|\n",
      "+---+-----+-----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.withColumn(\"Bonus\", col(\"Salary\") * 0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Renaming Columns\n",
    "\n",
    "Rename the `Age` column to `Employee_Age` using both SQL and DataFrame syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+------------+\n",
      "| ID| Name| Department|Salary|Employee_Age|\n",
      "+---+-----+-----------+------+------------+\n",
      "|  1|Alice|      Sales| 50000|          29|\n",
      "|  2|  Bob|Engineering| 60000|          35|\n",
      "|  3|Cathy|      Sales| 55000|          30|\n",
      "|  4|David|Engineering| 65000|          40|\n",
      "|  5|  Eva|  Marketing| 45000|          23|\n",
      "+---+-----+-----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"SELECT ID, Name, Department, Salary, Age AS Employee_Age FROM employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+------------+\n",
      "| ID| Name| Department|Salary|Employee_Age|\n",
      "+---+-----+-----------+------+------------+\n",
      "|  1|Alice|      Sales| 50000|          29|\n",
      "|  2|  Bob|Engineering| 60000|          35|\n",
      "|  3|Cathy|      Sales| 55000|          30|\n",
      "|  4|David|Engineering| 65000|          40|\n",
      "|  5|  Eva|  Marketing| 45000|          23|\n",
      "+---+-----+-----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame functions\n",
    "df.withColumnRenamed(\"Age\", \"Employee_Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1: Calculate Total Compensation\n",
    "\n",
    "Add a new column `Total_Compensation`, which is the sum of `Salary` and `Bonus`. Try this using both SQL and DataFrame syntax.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+------------------+\n",
      "| ID| Name| Department|Salary|Age|Total_Compensation|\n",
      "+---+-----+-----------+------+---+------------------+\n",
      "|  1|Alice|      Sales| 50000| 29|           55000.0|\n",
      "|  2|  Bob|Engineering| 60000| 35|           66000.0|\n",
      "|  3|Cathy|      Sales| 55000| 30|           60500.0|\n",
      "|  4|David|Engineering| 65000| 40|           71500.0|\n",
      "|  5|  Eva|  Marketing| 45000| 23|           49500.0|\n",
      "+---+-----+-----------+------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using SQL\n",
    "spark.sql(\"SELECT *, Salary + (Salary * 0.1) AS Total_Compensation FROM employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+---+------+------------------+\n",
      "| ID| Name| Department|Salary|Age| Bonus|Total_Compensation|\n",
      "+---+-----+-----------+------+---+------+------------------+\n",
      "|  1|Alice|      Sales| 50000| 29|5000.0|           55000.0|\n",
      "|  2|  Bob|Engineering| 60000| 35|6000.0|           66000.0|\n",
      "|  3|Cathy|      Sales| 55000| 30|5500.0|           60500.0|\n",
      "|  4|David|Engineering| 65000| 40|6500.0|           71500.0|\n",
      "|  5|  Eva|  Marketing| 45000| 23|4500.0|           49500.0|\n",
      "+---+-----+-----------+------+---+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using DataFrame functions\n",
    "df.withColumn(\"Bonus\", col(\"Salary\") * 0.1) \\\n",
    "  .withColumn(\"Total_Compensation\", col(\"Salary\") + col(\"Bonus\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2: Filter and Sort by Department and Age\n",
    "\n",
    "Filter for employees in the `Sales` department who are under 35, and then sort them by `Age` in ascending order. Try this using both SQL and DataFrame syntax.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+---+\n",
      "| ID| Name|Department|Salary|Age|\n",
      "+---+-----+----------+------+---+\n",
      "|  1|Alice|     Sales| 50000| 29|\n",
      "|  3|Cathy|     Sales| 55000| 30|\n",
      "+---+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using SQL\n",
    "spark.sql(\"SELECT * FROM employees WHERE Department = 'Sales' AND Age < 35 ORDER BY Age ASC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+---+\n",
      "| ID| Name|Department|Salary|Age|\n",
      "+---+-----+----------+------+---+\n",
      "|  1|Alice|     Sales| 50000| 29|\n",
      "|  3|Cathy|     Sales| 55000| 30|\n",
      "+---+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using DataFrame functions\n",
    "df.filter((col(\"Department\") == \"Sales\") & (col(\"Age\") < 35)) \\\n",
    "  .orderBy(col(\"Age\").asc()) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3: Group by Department and Calculate Total Salary\n",
    "\n",
    "Group employees by `Department` and calculate the total salary per department. Try this using both SQL and DataFrame syntax.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "| Department|Total_Salary|\n",
      "+-----------+------------+\n",
      "|      Sales|      105000|\n",
      "|Engineering|      125000|\n",
      "|  Marketing|       45000|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using SQL\n",
    "spark.sql(\"SELECT Department, SUM(Salary) AS Total_Salary FROM employees GROUP BY Department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "| Department|Total_Salary|\n",
      "+-----------+------------+\n",
      "|      Sales|      105000|\n",
      "|Engineering|      125000|\n",
      "|  Marketing|       45000|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution: Using DataFrame functions\n",
    "df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"Total_Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL-Only vs. DataFrame-Only Operations\n",
    "\n",
    "Here, we’ll explore the operations that can only be performed with SQL and those that can only be done with DataFrame functions.\n",
    "\n",
    "### SQL-Only Operations\n",
    "1. **Complex Joins with Multiple Conditions**: SQL is often simpler for multi-condition joins.\n",
    "   ```sql\n",
    "   SELECT a.Name, b.Department FROM employees a \n",
    "   JOIN departments b ON a.ID = b.ID AND a.Salary > b.Salary\n",
    "   ```\n",
    "2. **Window Functions**: SQL allows window functions with partitioning and ordering over subsets.\n",
    "   ```sql\n",
    "   SELECT Name, Salary, RANK() OVER(PARTITION BY Department ORDER BY Salary DESC) AS SalaryRank FROM employees\n",
    "   ```\n",
    "3. **Subqueries**: SQL supports nested queries, useful for filtering with complex conditions.\n",
    "   ```sql\n",
    "   SELECT * FROM employees WHERE Salary > (SELECT AVG(Salary) FROM employees)\n",
    "   ```\n",
    "\n",
    "### DataFrame-Only Operations\n",
    "1. **Custom UDFs (User-Defined Functions)**: DataFrames allow custom Python functions to be applied to columns.\n",
    "   ```python\n",
    "   from pyspark.sql.functions import udf\n",
    "   from pyspark.sql.types import IntegerType\n",
    "   \n",
    "   def add_bonus(salary):\n",
    "       return salary + 1000\n",
    "   \n",
    "   add_bonus_udf = udf(add_bonus, IntegerType())\n",
    "   df.withColumn(\"Adjusted_Salary\", add_bonus_udf(df[\"Salary\"])).show()\n",
    "   ```\n",
    "2. **Chaining Transformations**: DataFrame syntax is better for chaining multiple transformations.\n",
    "   ```python\n",
    "   df.withColumn(\"Bonus\", col(\"Salary\") * 0.1).filter(col(\"Age\") > 30).groupBy(\"Department\").count().show()\n",
    "   ```\n",
    "3. **Conditional Expressions**: Complex conditions can be handled with `when` and `otherwise` functions.\n",
    "   ```python\n",
    "   from pyspark.sql.functions import when\n",
    "   df.withColumn(\"High_Salary\", when(col(\"Salary\") > 60000, \"Yes\").otherwise(\"No\")).show()\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored how to perform common data manipulation tasks in Spark using both SQL and DataFrame functions. We highlighted unique capabilities that each approach offers:\n",
    "- SQL is powerful for complex queries involving joins, subqueries, and window functions.\n",
    "- DataFrames are preferred for transformations involving custom Python functions (UDFs) and chained operations.\n",
    "\n",
    "Choosing between SQL and DataFrame syntax in Spark often depends on the complexity of the operation, familiarity with SQL, and the need to integrate with Python-based transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
