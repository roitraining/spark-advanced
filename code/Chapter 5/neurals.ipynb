{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12665de3",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Classifier (MLPC) in PySpark\n",
    "\n",
    "This notebook demonstrates how to use the **Multilayer Perceptron Classifier (MLPC)** in PySpark for classification tasks.\n",
    "We will cover:\n",
    "1. Data preparation.\n",
    "2. Building an MLPC model.\n",
    "3. Training and evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003847ba",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Spark Session\n",
    "First, we create a Spark session to work with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8951a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLPC Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a51e1",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "We will create a simple dataset for binary classification. Each row represents features and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create sample dataset\n",
    "data = [\n",
    "    Row(features=[0.0, 0.0], label=0.0),\n",
    "    Row(features=[0.0, 1.0], label=1.0),\n",
    "    Row(features=[1.0, 0.0], label=1.0),\n",
    "    Row(features=[1.0, 1.0], label=0.0)\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df665c",
   "metadata": {},
   "source": [
    "## Step 3: Split Data into Training and Test Sets\n",
    "We split the dataset into 80% training and 20% testing to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b594f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344c04f",
   "metadata": {},
   "source": [
    "## Step 4: Build the MLPC Model\n",
    "We use PySpark's `MultilayerPerceptronClassifier` to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the model layers (input layer, hidden layers, output layer)\n",
    "# In this case: 2 input neurons, 2 hidden neurons, 2 output neurons (binary classification)\n",
    "layers = [2, 2, 2]\n",
    "\n",
    "# Initialize the MLPC model\n",
    "mlpc = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"label\", layers=layers, seed=1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b26139",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "Fit the MLPC model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29dd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = mlpc.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df5a7a",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions\n",
    "Use the trained model to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083afe9",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "Evaluate the model using metrics like accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d84a7a",
   "metadata": {},
   "source": [
    "## Step 8: Stop the Spark Session\n",
    "Finally, we stop the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
