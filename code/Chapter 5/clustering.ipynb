{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0554fe1f",
   "metadata": {},
   "source": [
    "# Spark K-Means Clustering Example\n",
    "\n",
    "This notebook demonstrates how to use the K-Means clustering algorithm in PySpark's MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ececf",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Spark Session\n",
    "First, we create a Spark session to work with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3db8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansClustering\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead417fa",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "We create a simple dataset for clustering. This dataset contains points in a 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a sample dataset\n",
    "data = [\n",
    "    Row(id=1, x=1.0, y=1.0),\n",
    "    Row(id=2, x=1.5, y=1.5),\n",
    "    Row(id=3, x=3.0, y=3.0),\n",
    "    Row(id=4, x=5.0, y=5.0),\n",
    "    Row(id=5, x=3.5, y=3.5),\n",
    "    Row(id=6, x=4.5, y=4.5)\n",
    "]\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9aa680",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "We use `VectorAssembler` to combine the feature columns (`x` and `y`) into a single vector column required by the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140567c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "dataset = assembler.transform(df)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31f90c",
   "metadata": {},
   "source": [
    "## Step 4: Apply K-Means Clustering\n",
    "We apply the K-Means algorithm to cluster the data points into two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Create and train the K-Means model\n",
    "kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28e726",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Clustering Model\n",
    "We evaluate the model by computing the **Within Set Sum of Squared Errors (WSSSE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing WSSSE\n",
    "wssse = model.computeCost(dataset)\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe402d3",
   "metadata": {},
   "source": [
    "## Step 6: Extract Cluster Centers\n",
    "We extract and display the cluster centers determined by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b76c0",
   "metadata": {},
   "source": [
    "## Step 7: Stop the Spark Session\n",
    "Finally, we stop the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212367bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29080a-279e-4007-8fe5-7ee57d18f434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
